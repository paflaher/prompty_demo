{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with prompty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Learning Objectives** - Upon completing this tutorial, you should be able to:\n",
    "\n",
    "- Write LLM application using prompty and visualize the trace of your application.\n",
    "- batch run prompty against multi lines of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\paflaher\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install promptflow-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Execute a Prompty\n",
    "\n",
    "Prompty is a file with .prompty extension for developing prompt template. \n",
    "The prompty asset is a markdown file with a modified front matter. \n",
    "The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "name: Basic Prompt\n",
      "description: A basic prompt that uses the chat API to answer questions\n",
      "model:\n",
      "    api: chat\n",
      "    configuration:\n",
      "        type: azure_openai\n",
      "        azure_deployment: gpt-4o\n",
      "    parameters:\n",
      "        max_tokens: 128\n",
      "        temperature: 0.2\n",
      "inputs:\n",
      "  question:\n",
      "    type: string\n",
      "sample:\n",
      "  \"question\": \"Who is the most famous person in the world?\"\n",
      "---\n",
      "system:\n",
      "You are an AI assistant who helps people find information.\n",
      "As the assistant, you answer questions briefly, succinctly. \n",
      "\n",
      "user:\n",
      "{{question}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"basic.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running below cell, please configure required environment variable `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` by create an `.env` file. Please refer to `../.env.example` as an template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv(\"credentials.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequiredEnvironmentVariablesNotSetError",
     "evalue": "Required environment variables ['AZURE_OPENAI_ENDPOINT', 'AZURE_OPENAI_API_KEY'] to build AzureOpenAIConnection not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequiredEnvironmentVariablesNotSetError\u001b[0m   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m f \u001b[38;5;241m=\u001b[39m Prompty\u001b[38;5;241m.\u001b[39mload(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic.prompty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# execute the flow as function\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of France?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\promptflow\\core\\_flow.py:395\u001b[0m, in \u001b[0;36mPrompty.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m enrich_prompt_template(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_template, variables\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# 1. Get connection\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_model_configuration_to_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# 2.deal with prompt\u001b[39;00m\n\u001b[0;32m    398\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\promptflow\\core\\_prompty_utils.py:91\u001b[0m, in \u001b[0;36mconvert_model_configuration_to_connection\u001b[1;34m(model_configuration)\u001b[0m\n\u001b[0;32m     89\u001b[0m     connection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_base\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_empty_connection_config(connection):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAzureOpenAIConnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AzureOpenAIConnection(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconnection)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\promptflow\\core\\_connection.py:268\u001b[0m, in \u001b[0;36mAzureOpenAIConnection.from_env\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    266\u001b[0m api_version \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequiredEnvironmentVariablesNotSetError(\n\u001b[0;32m    269\u001b[0m         env_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m], cls_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# Note: Do not pass api_version None when init class object as we have default version.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m optional_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: api_version} \u001b[38;5;28;01mif\u001b[39;00m api_version \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[1;31mRequiredEnvironmentVariablesNotSetError\u001b[0m: Required environment variables ['AZURE_OPENAI_ENDPOINT', 'AZURE_OPENAI_API_KEY'] to build AzureOpenAIConnection not set."
     ]
    }
   ],
   "source": [
    "from promptflow.core import Prompty\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Prompty.load(source=\"basic.prompty\")\n",
    "\n",
    "# execute the flow as function\n",
    "result = f(question=\"What is the capital of France?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override configuration with `AzureOpenAIModelConfiguration` and `OpenAIModelConfiguration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
    "\n",
    "# override configuration with AzureOpenAIModelConfiguration\n",
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    # azure_endpoint=\"${env:AZURE_OPENAI_ENDPOINT}\",  # Use ${env:<ENV_NAME>} to surround the environment variable name.\n",
    "    # api_key=\"${env:AZURE_OPENAI_API_KEY}\",\n",
    "    azure_deployment=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "# override configuration with OpenAIModelConfiguration\n",
    "# configuration = OpenAIModelConfiguration(\n",
    "#     base_url=\"${env:OPENAI_BASE_URL}\",\n",
    "#     api_key=\"${env:OPENAI_API_KEY}\",\n",
    "#     model=\"gpt-3.5-turbo\"\n",
    "# )\n",
    "\n",
    "override_model = {\"configuration\": configuration, \"parameters\": {\"max_tokens\": 512}}\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Prompty.load(source=\"basic.prompty\", model=override_model)\n",
    "\n",
    "# execute the flow as function\n",
    "result = f(question=\"What is the capital of France?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trace by using start_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run below cell will collect a trace in trace UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun the function, which will be recorded in the trace\n",
    "question = \"What is the capital of Japan?\"\n",
    "ground_truth = \"Tokyo\"\n",
    "result = f(question=question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval the result \n",
    "\n",
    "Note: the eval flow returns a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompty as a flow\n",
    "eval_flow = Prompty.load(\"../eval-basic/eval.prompty\")\n",
    "# execute the flow as function\n",
    "result = eval_flow(question=question, ground_truth=ground_truth, answer=result)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch run with multi-line data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# batch run requires promptflow-devkit package\n",
    "%pip install promptflow-devkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import PFClient\n",
    "\n",
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = \"./basic.prompty\"  # path to the prompty file\n",
    "data = \"./data.jsonl\"  # path to the data file\n",
    "\n",
    "# create run with the flow and data\n",
    "base_run = pf.run(\n",
    "    flow=flow,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        \"question\": \"${data.question}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate your flow\n",
    "Then you can use an evaluation method to evaluate your flow. The evaluation methods are also flows which usually using LLM assert the produced output matches certain expectation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the previous batch run\n",
    "The **base_run** is the batch run we completed in step 2 above, for web-classification flow with \"data.jsonl\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompty = \"../eval-basic/eval.prompty\"\n",
    "\n",
    "eval_run = pf.run(\n",
    "    flow=eval_prompty,\n",
    "    data=\"./data.jsonl\",  # path to the data file\n",
    "    run=base_run,  # specify base_run as the run you want to evaluate\n",
    "    column_mapping={\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"answer\": \"${run.outputs.output}\",  # TODO refine this mapping\n",
    "        \"ground_truth\": \"${data.ground_truth}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf.get_details(eval_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize run using ui\n",
    "pf.visualize([base_run, eval_run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "By now you've successfully run your first prompt flow and even did evaluation on it. That's great!\n",
    "\n",
    "You can check out more examples:\n",
    "- [Basic Chat](https://github.com/microsoft/promptflow/tree/main/examples/prompty/chat-basic): demonstrates how to create a chatbot that can remember previous interactions and use the conversation history to generate next message."
   ]
  }
 ],
 "metadata": {
  "build_doc": {
   "author": [
    "lalala123123@github.com",
    "wangchao1230@github.com"
   ],
   "category": "local",
   "section": "Prompty",
   "weight": 10
  },
  "description": "A quickstart tutorial to run a prompty and evaluate it.",
  "kernelspec": {
   "display_name": "prompt_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "resources": "examples/requirements.txt, examples/prompty/basic, examples/prompty/eval-basic"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
